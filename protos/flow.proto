syntax = "proto3";

import "google/protobuf/timestamp.proto";
import "peers.proto";

package peerdb_flow;

message TableNameMapping {
  string source_table_name = 1;
  string destination_table_name = 2;
}

message RelationMessageColumn {
  uint32 flags = 1;
  string name = 2;
  uint32 data_type = 3;
}

message RelationMessage {
  uint32 relation_id = 1;
  string relation_name = 2;
  repeated RelationMessageColumn columns = 3;
}

message TableMapping {
  string source_table_identifier = 1;
  string destination_table_identifier = 2;
  string partition_key = 3;
  repeated string exclude = 4;
}

message FlowConnectionConfigs {
  peerdb_peers.Peer source = 1;
  peerdb_peers.Peer destination = 2;
  string flow_job_name = 3;
  TableSchema table_schema = 4;
  repeated TableMapping table_mappings = 5;
  map<uint32, string> src_table_id_name_mapping = 6;
  map<string, TableSchema> table_name_schema_mapping = 7;

  // This is an optional peer that will be used to hold metadata in cases where
  // the destination isn't ideal for holding metadata.
  peerdb_peers.Peer metadata_peer = 8;
  uint32 max_batch_size = 9;
  bool do_initial_copy = 10;

  string publication_name = 11;
  uint32 snapshot_num_rows_per_partition = 12;

  // max parallel workers is per table
  uint32 snapshot_max_parallel_workers = 13;
  uint32 snapshot_num_tables_in_parallel = 14;
  QRepSyncMode snapshot_sync_mode = 15;
  QRepSyncMode cdc_sync_mode = 16;
  string snapshot_staging_path = 17;
  string cdc_staging_path = 18;

  // currently only works for snowflake
  bool soft_delete = 19;

  string replication_slot_name = 20;

  // the below two are for eventhub only
  int64 push_batch_size = 21;
  int64 push_parallelism = 22;

  // if true, then the flow will be resynced
  // create new tables with "_resync" suffix, perform initial load and then swap the new tables with the old ones
  // to be used after the old mirror is dropped
  bool resync = 23;

  string soft_delete_col_name = 24;
  string synced_at_col_name = 25;
}

message RenameTableOption {
  string current_name = 1;
  string new_name = 2;
  TableSchema table_schema = 3;
}

message RenameTablesInput {
  string flow_job_name = 1;
  peerdb_peers.Peer peer = 2;
  repeated RenameTableOption rename_table_options = 3;
  optional string soft_delete_col_name = 4;
}

message RenameTablesOutput {
  string flow_job_name = 1;
}

message CreateTablesFromExistingInput {
  string flow_job_name = 1;
  peerdb_peers.Peer peer = 2;
  map<string, string> new_to_existing_table_mapping = 3;
}

message CreateTablesFromExistingOutput {
  string flow_job_name = 2;
}

message SyncFlowOptions {
  int32 batch_size = 1;
  map<uint32, RelationMessage> relation_message_mapping = 2;
}

message NormalizeFlowOptions {
  int32 batch_size = 1;
}

message LastSyncState {
  int64 checkpoint = 1;
  google.protobuf.Timestamp last_synced_at = 2;
}

message StartFlowInput {
  LastSyncState last_sync_state = 1;
  FlowConnectionConfigs flow_connection_configs = 2;
  SyncFlowOptions sync_flow_options = 3;
  map<uint32, RelationMessage> relation_message_mapping = 4;
}

message StartNormalizeInput {
  FlowConnectionConfigs flow_connection_configs = 1;
}

message GetLastSyncedIDInput {
  peerdb_peers.Peer peer_connection_config = 1;
  string flow_job_name = 2;
}

message EnsurePullabilityInput {
  peerdb_peers.Peer peer_connection_config = 1;
  string flow_job_name = 2;
  string source_table_identifier = 3;
}

message EnsurePullabilityBatchInput {
  peerdb_peers.Peer peer_connection_config = 1;
  string flow_job_name = 2;
  repeated string source_table_identifiers = 3;
}

message PostgresTableIdentifier {
  uint32 rel_id = 1;
}

message TableIdentifier {
  oneof table_identifier {
    PostgresTableIdentifier postgres_table_identifier = 1;
  }
}

message EnsurePullabilityOutput {
  TableIdentifier table_identifier = 1;
}

message EnsurePullabilityBatchOutput {
  map<string, TableIdentifier> table_identifier_mapping = 1;
}

message SetupReplicationInput {
  peerdb_peers.Peer peer_connection_config = 1;
  string flow_job_name = 2;
  map<string, string> table_name_mapping = 3;
  // replicate to destination using ctid
  peerdb_peers.Peer destination_peer = 4;
  bool do_initial_copy = 5;
  string existing_publication_name = 6;
  string existing_replication_slot_name = 7;
}

message SetupReplicationOutput {
  string slot_name = 1;
  string snapshot_name = 2;
}

message CreateRawTableInput {
  peerdb_peers.Peer peer_connection_config = 1;
  string flow_job_name = 2;
  map<string, string> table_name_mapping = 3;
  QRepSyncMode cdc_sync_mode = 4;
}

message CreateRawTableOutput { string table_identifier = 1; }

message TableSchema {
  string table_identifier = 1;
  // list of column names and types, types can be one of the following:
  // "string", "int", "float", "bool", "timestamp".
  map<string, string> columns = 2;
  repeated string primary_key_columns = 3;
  bool is_replica_identity_full = 4;
}

message GetTableSchemaBatchInput {
  peerdb_peers.Peer peer_connection_config = 1;
  repeated string table_identifiers = 2;
}

message GetTableSchemaBatchOutput {
  map<string, TableSchema> table_name_schema_mapping = 1;
}

message SetupNormalizedTableInput {
  peerdb_peers.Peer peer_connection_config = 1;
  string table_identifier = 2;
  TableSchema source_table_schema = 3;
}

message SetupNormalizedTableBatchInput {
  peerdb_peers.Peer peer_connection_config = 1;
  map<string, TableSchema> table_name_schema_mapping = 2;

  // migration related columns
  string soft_delete_col_name = 4;
  string synced_at_col_name = 5;
}

message SetupNormalizedTableOutput {
  string table_identifier = 1;
  bool already_exists = 2;
}

message SetupNormalizedTableBatchOutput {
  map<string, bool> table_exists_mapping = 1;
}

// partition ranges [start, end] inclusive
message IntPartitionRange {
  int64 start = 1;
  int64 end = 2;
}

message TimestampPartitionRange {
  google.protobuf.Timestamp start = 1;
  google.protobuf.Timestamp end = 2;
}

message TID {
  uint32 block_number = 1;
  uint32 offset_number = 2;
}

message TIDPartitionRange {
  TID start = 1;
  TID end = 2;
}

message PartitionRange {
  // can be a timestamp range or an integer range
  oneof range {
    IntPartitionRange int_range = 1;
    TimestampPartitionRange timestamp_range = 2;
    TIDPartitionRange tid_range = 3;
  }
}

// protos for qrep
enum QRepSyncMode {
  QREP_SYNC_MODE_MULTI_INSERT = 0;
  QREP_SYNC_MODE_STORAGE_AVRO = 1;
}

enum QRepWriteType {
  QREP_WRITE_MODE_APPEND = 0;
  QREP_WRITE_MODE_UPSERT = 1;
  // only valid when initial_copy_true is set to true. TRUNCATES tables before reverting to APPEND.
  QREP_WRITE_MODE_OVERWRITE = 2;
}

message QRepWriteMode {
  QRepWriteType write_type = 1;
  repeated string upsert_key_columns = 2;
}

message QRepConfig {
  string flow_job_name = 1;

  peerdb_peers.Peer source_peer = 2;
  peerdb_peers.Peer destination_peer = 3;

  string destination_table_identifier = 4;

  string query = 5;

  string watermark_table = 6;
  string watermark_column = 7;

  bool initial_copy_only = 8;
  QRepSyncMode sync_mode = 9;

  // DEPRECATED: eliminate when breaking changes are allowed.
  uint32 batch_size_int = 10;
  // DEPRECATED: eliminate when breaking changes are allowed.
  uint32 batch_duration_seconds = 11;

  uint32 max_parallel_workers = 12;

  // time to wait between getting partitions to process
  uint32 wait_between_batches_seconds = 13;

  QRepWriteMode write_mode = 14;

  // This is only used when sync_mode is AVRO
  // this is the location where the avro files will be written
  // if this starts with gs:// then it will be written to GCS
  // if this starts with s3:// then it will be written to S3
  // if nothing is specified then it will be written to local disk, only supported in Snowflake
  // if using GCS or S3 make sure your instance has the correct permissions.
  string staging_path = 15;

  // This setting overrides batch_size_int and batch_duration_seconds
  // and instead uses the number of rows per partition to determine
  // how many rows to process per batch.
  uint32 num_rows_per_partition = 16;

  // Creates the watermark table on the destination as-is, can be used for some queries.
  bool setup_watermark_table_on_destination = 17;

  // create new tables with "_peerdb_resync" suffix, perform initial load and then swap the new table with the old ones
  // to be used after the old mirror is dropped
  bool dst_table_full_resync = 18;
}

message QRepPartition {
  string partition_id = 2;
  PartitionRange range = 3;
  bool full_table_partition = 4;
}

message QRepPartitionBatch {
  int32 batch_id = 1;
  repeated QRepPartition partitions = 2;
}

message QRepParitionResult {
  repeated QRepPartition partitions = 1;
}

message DropFlowInput {
  string flow_name = 1;
}

message DeltaAddedColumn {
  string column_name = 1;
  string column_type = 2;
}

message TableSchemaDelta {
  string src_table_name = 1;
  string dst_table_name = 2;
  repeated DeltaAddedColumn added_columns = 3;
}

message ReplayTableSchemaDeltaInput {
  FlowConnectionConfigs flow_connection_configs = 1;
  repeated TableSchemaDelta table_schema_deltas = 2;
}

message QRepFlowState {
  QRepPartition last_partition = 1;
  uint64 num_partitions_processed = 2;
  bool needs_resync = 3;
}
